{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import Common\n",
    "from pyspark import *\n",
    "from pyspark.streaming import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=myapp>\n",
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "common = Common()\n",
    "sc = common.get_spark_core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  484\n",
      "First:  Think of it for a moment – 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.\n"
     ]
    }
   ],
   "source": [
    "file = '/home/ec2-user/data/blogtexts'\n",
    "rdd = sc.textFile(file)\n",
    "print ('Count: ', rdd.count())\n",
    "print ('First: ', rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  484\n",
      "First:  ['think', 'of', 'it', 'for', 'a', 'moment', '–', '1', 'qunitillion', '=', '1', 'million', 'billion!', 'can', 'you', 'imagine', 'how', 'many', 'drives', '/', 'cds', '/', 'blue-ray', 'dvds', 'would', 'be', 'required', 'to', 'store', 'them?', 'it', 'is', 'difficult', 'to', 'imagine', 'this', 'scale', 'of', 'data', 'generation', 'even', 'as', 'a', 'data', 'science', 'professional.', 'while', 'this', 'pace', 'of', 'data', 'generation', 'is', 'very', 'exciting,', 'it', 'has', 'created', 'entirely', 'new', 'set', 'of', 'challenges', 'and', 'has', 'forced', 'us', 'to', 'find', 'new', 'ways', 'to', 'handle', 'big', 'huge', 'data', 'effectively.']\n"
     ]
    }
   ],
   "source": [
    "def lowercase(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines\n",
    "rdd1 = rdd.map(lowercase)\n",
    "print ('Count: ', rdd1.count())\n",
    "print ('First: ', rdd1.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  5304\n",
      "First 5:  ['think', 'of', 'it', 'for', 'a']\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.flatMap(lambda x:x)\n",
    "print ('Count: ', rdd2.count())\n",
    "print ('First 5: ', rdd2.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  3860\n",
      "First 5:  ['think', 'of', 'it', 'moment', '1']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "stopwords = ['is','am','are','the','for','a']\n",
    "rdd3 = rdd2.filter(lambda x: x not in stopwords).filter(lambda x : re.match(\"^[A-Za-z0-9]*$\", x))\n",
    "print ('Count: ', rdd3.count())\n",
    "print ('First 5: ', rdd3.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  467\n",
      "First 5:  [('thi', <pyspark.resultiterable.ResultIterable object at 0x7febfc4b7250>), ('of', <pyspark.resultiterable.ResultIterable object at 0x7febfc4b7090>), ('it', <pyspark.resultiterable.ResultIterable object at 0x7febfc4bc3d0>), ('mom', <pyspark.resultiterable.ResultIterable object at 0x7febfc4b53d0>), ('1', <pyspark.resultiterable.ResultIterable object at 0x7febfc4b7290>)]\n",
      "\n",
      "First 3 letter group: thi ['think', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'think', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'things', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'things', 'this', 'this', 'this']\n"
     ]
    }
   ],
   "source": [
    "rdd4 = rdd3.groupBy(lambda w: w[0:3])\n",
    "print ('Count: ', rdd4.count())\n",
    "print ('First 5: ', rdd4.take(5))\n",
    "print ('\\nFirst 3 letter group:', rdd4.first()[0], list(rdd4.first()[1]))\n",
    "# print ('\\tTokens', list(rdd4.first()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 164), ('in', 143), ('of', 122), ('and', 106), ('we', 103), ('spark', 69)]\n",
      "Spark counter: [('spark', 69)]\n"
     ]
    }
   ],
   "source": [
    "rdd3_mapped = rdd3.map(lambda x: (x,1))\n",
    "rdd3_reduced = rdd3_mapped.reduceByKey(lambda x,y : x+y).sortBy(lambda a: -a[1])\n",
    "print (rdd3_reduced.take(6))\n",
    "\n",
    "print ('Spark counter:', rdd3_reduced.filter(lambda x : x[0] == 'spark').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Partitions: 1\n",
      "After Partitions: 10\n",
      "First:  them\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 7, 6, 12, 8, 6, 8, 5, 10, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = rdd3.getNumPartitions()\n",
    "print ('Before Partitions:', N)\n",
    "rdd_repart = rdd3.repartition(10)\n",
    "N = rdd_repart.getNumPartitions()\n",
    "print ('After Partitions:', N)\n",
    "print ('First: ', rdd_repart.first())\n",
    "\n",
    "def func(iterator):\n",
    "    yield list(iterator).count('spark')\n",
    "\n",
    "rdd_repart.mapPartitions(func).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      "\t First 5: [('in', 143), ('in', 143), ('we', 103), ('spark', 69), ('this', 64)]\n",
      "\t count:  462\n",
      "Sample 2\n",
      "\t First 5: [('this', 64), ('this', 64), ('can', 55), ('apache', 52), ('apache', 52)]\n",
      "\t count:  434\n"
     ]
    }
   ],
   "source": [
    "sample1 = rdd3_reduced.sample(withReplacement=True, fraction=0.5)\n",
    "sample2 = rdd3_reduced.sample(withReplacement=True, fraction=0.5)\n",
    "\n",
    "print (\"Sample 1\")\n",
    "print ('\\t First 5:', sample1.take(5))\n",
    "print ('\\t count: ', sample1.count())\n",
    "print (\"Sample 2\")\n",
    "print ('\\t First 5:', sample2.take(5))\n",
    "print ('\\t count: ', sample2.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNION\n",
      "\t First 5: [('in', 143), ('in', 143), ('we', 103), ('spark', 69), ('this', 64)]\n",
      "\t count:  896\n"
     ]
    }
   ],
   "source": [
    "print (\"UNION\")\n",
    "union_1_2 = sample1.union(sample2)\n",
    "print ('\\t First 5:', union_1_2.take(5))\n",
    "print ('\\t count: ', union_1_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOIN\n",
      "\t First 5: [('this', (64, 64)), ('this', (64, 64)), ('an', (16, 16)), ('an', (16, 16)), ('step', (14, 14))]\n",
      "\t count:  257\n"
     ]
    }
   ],
   "source": [
    "print (\"JOIN\")\n",
    "join_1_2 = sample1.join(sample2)\n",
    "print ('\\t First 5:', join_1_2.take(5))\n",
    "print ('\\t count: ', join_1_2.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
      "sum: 21 count: 6\n",
      "avg: 3.5\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd = rdd.map(lambda x : (x,1))\n",
    "print (rdd.collect())\n",
    "\n",
    "(_sum,_count) = rdd.reduce(lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "print (\"sum:\", _sum, \"count:\", _count)\n",
    "print (\"avg:\", _sum/_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (3, 1), (5, 1), (6, 1)]\n",
      "4.0\n",
      "Standard deviation:  1.5811388300841898\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "data = [2,3,5,6]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd = rdd.map(lambda x : (x,1))\n",
    "print (rdd.collect())\n",
    "\n",
    "(_sum,_count) = rdd.reduce(lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "_mean = _sum/_count\n",
    "print (_mean)\n",
    "\n",
    "_mean_square = rdd.map(lambda x: ((x[0]-_mean) ** 2) )\n",
    "_avg_mean_square = (_mean_square.reduce(lambda x,y: x+y))/rdd.count()\n",
    "_std = math.sqrt(_avg_mean_square)\n",
    "print ('Standard deviation: ', _std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd: 500\n",
      "rdd2: 138\n",
      "[9, 12, 15, 17, 18, 29, 31, 33, 34, 37, 41, 46, 48, 50, 53, 57, 63, 65, 77, 80, 84, 85, 87, 91, 93, 94, 97, 109, 112, 115, 118, 125, 130, 133, 135, 139, 140, 141, 152, 153, 154, 157, 158, 160, 161, 168, 172, 174, 176, 179, 182, 183, 186, 188, 192, 194, 197, 199, 204, 208, 214, 217, 220, 228, 233, 234, 235, 238, 244, 247, 249, 253, 260, 267, 268, 270, 277, 281, 282, 284, 286, 287, 293, 294, 298, 304, 311, 317, 320, 321, 325, 331, 332, 333, 355, 358, 359, 361, 363, 364, 366, 367, 368, 374, 382, 384, 385, 392, 394, 401, 411, 417, 422, 426, 427, 430, 433, 445, 449, 451, 452, 454, 466, 472, 473, 476, 478, 480, 492, 493]\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "rdd = sc.parallelize(range(0,500))\n",
    "count = rdd.count()\n",
    "print ('rdd:', count)\n",
    "\n",
    "fraction=0.3\n",
    "def filter_func(x):\n",
    "    seed(1)\n",
    "    for _ in range(count):\n",
    "        if random() <= fraction:\n",
    "            return x\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "rdd2 = rdd.filter(filter_func)\n",
    "print ('rdd2:', rdd2.count())\n",
    "print (rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

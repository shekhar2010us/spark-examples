{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import Common\n",
    "from pyspark import *\n",
    "from pyspark.streaming import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=myapp>\n",
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "common = Common()\n",
    "sc = common.get_sc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  484\n",
      "First:  Think of it for a moment – 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.\n"
     ]
    }
   ],
   "source": [
    "file = '/home/ec2-user/data/blogtexts'\n",
    "rdd = sc.textFile(file)\n",
    "print ('Count: ', rdd.count())\n",
    "print ('First: ', rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  484\n",
      "First:  ['think', 'of', 'it', 'for', 'a', 'moment', '–', '1', 'qunitillion', '=', '1', 'million', 'billion!', 'can', 'you', 'imagine', 'how', 'many', 'drives', '/', 'cds', '/', 'blue-ray', 'dvds', 'would', 'be', 'required', 'to', 'store', 'them?', 'it', 'is', 'difficult', 'to', 'imagine', 'this', 'scale', 'of', 'data', 'generation', 'even', 'as', 'a', 'data', 'science', 'professional.', 'while', 'this', 'pace', 'of', 'data', 'generation', 'is', 'very', 'exciting,', 'it', 'has', 'created', 'entirely', 'new', 'set', 'of', 'challenges', 'and', 'has', 'forced', 'us', 'to', 'find', 'new', 'ways', 'to', 'handle', 'big', 'huge', 'data', 'effectively.']\n"
     ]
    }
   ],
   "source": [
    "def lowercase(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines\n",
    "rdd1 = rdd.map(lowercase)\n",
    "print ('Count: ', rdd1.count())\n",
    "print ('First: ', rdd1.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  5304\n",
      "First 5:  ['think', 'of', 'it', 'for', 'a']\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.flatMap(lambda x:x)\n",
    "print ('Count: ', rdd2.count())\n",
    "print ('First 5: ', rdd2.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  3860\n",
      "First 5:  ['think', 'of', 'it', 'moment', '1']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "stopwords = ['is','am','are','the','for','a']\n",
    "rdd3 = rdd2.filter(lambda x: x not in stopwords).filter(lambda x : re.match(\"^[A-Za-z0-9]*$\", x))\n",
    "print ('Count: ', rdd3.count())\n",
    "print ('First 5: ', rdd3.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  467\n",
      "First 5:  [('thi', <pyspark.resultiterable.ResultIterable object at 0x7f73a962a190>), ('of', <pyspark.resultiterable.ResultIterable object at 0x7f73a962ab50>), ('it', <pyspark.resultiterable.ResultIterable object at 0x7f73a9513110>), ('mom', <pyspark.resultiterable.ResultIterable object at 0x7f73a938ffd0>), ('1', <pyspark.resultiterable.ResultIterable object at 0x7f73a962a3d0>)]\n",
      "\n",
      "First 3 letter group: thi ['think', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'think', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'things', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'things', 'this', 'this', 'this']\n"
     ]
    }
   ],
   "source": [
    "rdd4 = rdd3.groupBy(lambda w: w[0:3])\n",
    "print ('Count: ', rdd4.count())\n",
    "print ('First 5: ', rdd4.take(5))\n",
    "print ('\\nFirst 3 letter group:', rdd4.first()[0], list(rdd4.first()[1]))\n",
    "# print ('\\tTokens', list(rdd4.first()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 164), ('in', 143), ('of', 122), ('and', 106), ('we', 103), ('spark', 69)]\n",
      "Spark counter: [('spark', 69)]\n"
     ]
    }
   ],
   "source": [
    "rdd3_mapped = rdd3.map(lambda x: (x,1))\n",
    "rdd3_reduced = rdd3_mapped.reduceByKey(lambda x,y : x+y).sortBy(lambda a: -a[1])\n",
    "print (rdd3_reduced.take(6))\n",
    "\n",
    "print ('Spark counter:', rdd3_reduced.filter(lambda x : x[0] == 'spark').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Partitions: 1\n",
      "After Partitions: 10\n",
      "First:  them\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 7, 6, 12, 8, 6, 8, 5, 10, 2]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = rdd3.getNumPartitions()\n",
    "print ('Before Partitions:', N)\n",
    "rdd_repart = rdd3.repartition(10)\n",
    "N = rdd_repart.getNumPartitions()\n",
    "print ('After Partitions:', N)\n",
    "print ('First: ', rdd_repart.first())\n",
    "\n",
    "def func(iterator):\n",
    "    yield list(iterator).count('spark')\n",
    "\n",
    "rdd_repart.mapPartitions(func).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      "\t First 5: [('in', 143), ('of', 122), ('can', 55), ('can', 55), ('it', 40)]\n",
      "\t count:  495\n",
      "Sample 2\n",
      "\t First 5: [('in', 143), ('of', 122), ('and', 106), ('we', 103), ('apache', 52)]\n",
      "\t count:  462\n"
     ]
    }
   ],
   "source": [
    "sample1 = rdd3_reduced.sample(withReplacement=True, fraction=0.5)\n",
    "sample2 = rdd3_reduced.sample(withReplacement=True, fraction=0.5)\n",
    "\n",
    "print (\"Sample 1\")\n",
    "print ('\\t First 5:', sample1.take(5))\n",
    "print ('\\t count: ', sample1.count())\n",
    "print (\"Sample 2\")\n",
    "print ('\\t First 5:', sample2.take(5))\n",
    "print ('\\t count: ', sample2.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNION\n",
      "\t First 5: [('in', 143), ('of', 122), ('can', 55), ('can', 55), ('it', 40)]\n",
      "\t count:  957\n"
     ]
    }
   ],
   "source": [
    "print (\"UNION\")\n",
    "union_1_2 = sample1.union(sample2)\n",
    "print ('\\t First 5:', union_1_2.take(5))\n",
    "print ('\\t count: ', union_1_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOIN\n",
      "\t First 5: [('in', (143, 143)), ('of', (122, 122)), ('an', (16, 16)), ('an', (16, 16)), ('an', (16, 16))]\n",
      "\t count:  262\n"
     ]
    }
   ],
   "source": [
    "print (\"JOIN\")\n",
    "join_1_2 = sample1.join(sample2)\n",
    "print ('\\t First 5:', join_1_2.take(5))\n",
    "print ('\\t count: ', join_1_2.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
